{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDZmI29tJz7c",
        "outputId": "f8e39abd-64c2-47b4-f4d1-eb9267e2217b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù User Input: Generate a prompt for a landscape painting.\n",
            "\n",
            "üñºÔ∏è Generated Image Prompt: generate image prompt: Generate a prompt prompt for a landscape painting.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Load a T5 model fine-tuned for generating descriptive prompts (or use GPT-3-like models)\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Input: user request for a scene description\n",
        "user_input = \"Generate a prompt for a landscape painting.\"\n",
        "\n",
        "# Format the prompt generation task for the model\n",
        "input_text = f\"generate image prompt: {user_input}\"\n",
        "\n",
        "# Tokenize the input text\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"tf\")\n",
        "\n",
        "# Generate the image prompt\n",
        "outputs = model.generate(input_ids, max_length=50, num_beams=4, early_stopping=True)\n",
        "generated_prompt = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Display result\n",
        "print(\"üìù User Input:\", user_input)\n",
        "print(\"\\nüñºÔ∏è Generated Image Prompt:\", generated_prompt)\n"
      ]
    }
  ]
}