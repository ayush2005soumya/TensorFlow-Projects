{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "201353e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec76f79c",
   "metadata": {},
   "source": [
    "# Sample sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cd0b721",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"TensorFlow is an end-to-end open-source platform for machine learning.\",\n",
    "    \"Natural Language Processing is a fascinating field.\",\n",
    "    \"Tokenization is the first step in NLP pipelines.\",\n",
    "    \"Subword tokenization helps with rare words.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be332729",
   "metadata": {},
   "source": [
    "# Build SubwordTextEncoder from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbea41fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    corpus, target_vocab_size=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981a97e8",
   "metadata": {},
   "source": [
    "# Print vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1758504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subword vocabulary size: 288\n"
     ]
    }
   ],
   "source": [
    "print(\"Subword vocabulary size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd357306",
   "metadata": {},
   "source": [
    "# Encode and decode a test sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9adc43f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"Subword tokenization is powerful for text models.\"\n",
    "encoded = tokenizer.encode(test_sentence)\n",
    "decoded = tokenizer.decode(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d771dc9",
   "metadata": {},
   "source": [
    "# Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ded14f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Sentence:\n",
      " Subword tokenization is powerful for text models.\n",
      "\n",
      "Encoded Tokens:\n",
      " [27, 4, 1, 144, 143, 151, 133, 146, 134, 149, 140, 64, 17, 148, 133, 152, 148, 64, 141, 143, 132, 133, 140, 147, 78]\n",
      "\n",
      "Decoded Sentence:\n",
      " Subword tokenization is powerful for text models.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOriginal Sentence:\\n\", test_sentence)\n",
    "print(\"\\nEncoded Tokens:\\n\", encoded)\n",
    "print(\"\\nDecoded Sentence:\\n\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c8de94",
   "metadata": {},
   "source": [
    "# Optional: visualize subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "814afe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subword Tokens:\n",
      "['Subword ', 'tokenization ', 'is ', 'p', 'o', 'w', 'e', 'r', 'f', 'u', 'l', ' ', 'for ', 't', 'e', 'x', 't', ' ', 'm', 'o', 'd', 'e', 'l', 's', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSubword Tokens:\")\n",
    "print([tokenizer.decode([token]) for token in encoded])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
