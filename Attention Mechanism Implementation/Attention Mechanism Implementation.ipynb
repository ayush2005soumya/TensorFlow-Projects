{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f62b0f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30793b9e",
   "metadata": {},
   "source": [
    "# Simulated encoder outputs (batch_size=1, time_steps=4, hidden_dim=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6e05ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = tf.random.normal([1, 4, 8])            # 4 encoder hidden states\n",
    "decoder_hidden_state = tf.random.normal([1, 8])          # Current decoder hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48cb29b",
   "metadata": {},
   "source": [
    "# Define basic attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea3cde66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(8)               # Linear for encoder outputs\n",
    "        self.W2 = tf.keras.layers.Dense(8)               # Linear for decoder state\n",
    "        self.V = tf.keras.layers.Dense(1)                # Scoring layer\n",
    " \n",
    "    def call(self, encoder_outputs, decoder_hidden):\n",
    "        decoder_hidden_exp = tf.expand_dims(decoder_hidden, 1)          # Expand for broadcasting\n",
    "        score = self.V(tf.nn.tanh(self.W1(encoder_outputs) + self.W2(decoder_hidden_exp)))  # Score eij\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)                # Normalize scores\n",
    "        context_vector = attention_weights * encoder_outputs            # Weight encoder outputs\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)          # Sum over time\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31af97b8",
   "metadata": {},
   "source": [
    "# Instantiate and compute attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "508fca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = BasicAttention()\n",
    "context_vector, attention_weights = attention(encoder_outputs, decoder_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08affb8a",
   "metadata": {},
   "source": [
    "# Print shapes and example values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8335e7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder outputs shape: (1, 4, 8)\n",
      "Decoder hidden state shape: (1, 8)\n",
      "Context vector shape: (1, 8)\n",
      "Attention weights shape: (1, 4, 1)\n",
      "Attention weights: [0.20486507 0.13106501 0.32616612 0.3379038 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Decoder hidden state shape:\", decoder_hidden_state.shape)\n",
    "print(\"Context vector shape:\", context_vector.shape)\n",
    "print(\"Attention weights shape:\", attention_weights.shape)\n",
    "print(\"Attention weights:\", tf.squeeze(attention_weights).numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
